from math import exp
import torch
from torch import nn
from torch.autograd import Variable
import torch.nn.functional as F

def bce_loss(pred,mask):
    pred=torch.sigmoid(pred)
    bce_loss=nn.BCELoss()
    bce=bce_loss(pred,mask)
    return bce

def iou_loss(pred,mask):
    pred=torch.sigmoid(pred)

    inter=(pred*mask).sum(dim=(2,3))
    union=(pred+mask).sum(dim=(2,3))

    iou=1-(inter+1)/(union-inter+1)
    iou=iou.mean()
    return iou

def gaussian(window_size, sigma):
    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])
    return gauss/gauss.sum()

def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())
    return window

def _ssim(img1, img2, window, window_size, channel, size_average = True):
    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)
    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1*mu2

    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq
    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq
    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2

    C1 = 0.01**2
    C2 = 0.03**2

    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))

    if size_average:
        return 1-ssim_map.mean()
    else:
        return 1-ssim_map.mean(1).mean(1).mean(1)


class SSIM(torch.nn.Module):
    def __init__(self, window_size=11, size_average=True):
        super(SSIM, self).__init__()
        self.window_size = window_size
        self.size_average = size_average
        self.channel = 1
        self.window = create_window(window_size, self.channel)

    def forward(self, img1, img2):
        (_, channel, _, _) = img1.size()

        if channel == self.channel and self.window.data.type() == img1.data.type():
            window = self.window
        else:
            window = create_window(self.window_size, channel)

            if img1.is_cuda:
                window = window.cuda(img1.get_device())
            window = window.type_as(img1)

            self.window = window
            self.channel = channel

        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)


def ssim_loss(img1, img2, window_size=11, size_average=True):
    (_, channel, _, _) = img1.size()
    window = create_window(window_size, channel)

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)

    return _ssim(img1, img2, window, window_size, channel, size_average)


class BinaryDiceLoss(nn.Module):
    def __init__(self):
        super(BinaryDiceLoss, self).__init__()

    def forward(self, input, targets):
        # Get the size of each batch N
        N = targets.size()[0]
        # Smoothing variable
        smooth = 1
        # Reshape width and height to the same dimension
        input_flat = input.view(N, -1)
        targets_flat = targets.view(N, -1)

        # Calculate intersection
        intersection = input_flat * targets_flat
        dice_eff = (2 * intersection.sum(1) + smooth) / (input_flat.sum(1) + targets_flat.sum(1) + smooth)
        # Calculate the average loss for each image in a batch
        loss = 1 - dice_eff.sum() / N
        return loss

def dice_loss(pred,mask):
    pred=torch.sigmoid(pred)
    d_loss=BinaryDiceLoss()
    dl=d_loss(pred,mask)
    return dl
# def dice_loss(pred, mask, smooth=1.):
#     pred = torch.sigmoid(pred)
#     # mask = mask.contiguous()
#
#     intersection = (pred * mask).sum(dim=2).sum(dim=2)
#
#     loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + mask.sum(dim=2).sum(dim=2) + smooth)))
#
#     return loss.mean()

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2, logits=False, reduce=True):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits
        self.reduce = reduce

    def forward(self, inputs, targets):
        if self.logits:
            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)
        else:
            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

        if self.reduce:
            return torch.mean(F_loss)
        else:
            return F_loss

def fl_loss(pred,mask):
    pred=torch.sigmoid(pred)
    fl_loss=FocalLoss()
    fl=fl_loss(pred,mask)
    return fl

def total_loss(pred,mask):
    pred1=torch.sigmoid(pred)
    l0=0.8
    l1=0.1
    l2=0.1
    loss=l0*fl_loss(pred,mask)+l1*dice_loss(pred,mask)+l2*ssim_loss(pred1,mask)
    return loss